{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2b4581-dab2-4d7c-8360-9fada2391c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import fiftyone.brain as fob\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone as fo \n",
    "\n",
    "fo.config.dataset_zoo_dir = \"./fiftyone_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32e5b408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 'train' already downloaded\n",
      "Split 'test' already downloaded\n",
      "Loading existing dataset 'mnist'. To reload from disk, either delete the existing dataset or provide a custom `dataset_name` to use\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method SampleCollection.get_annotation_info of Name:        mnist\n",
       "Media type:  image\n",
       "Num samples: 70000\n",
       "Persistent:  False\n",
       "Tags:        []\n",
       "Sample fields:\n",
       "    id:               fiftyone.core.fields.ObjectIdField\n",
       "    filepath:         fiftyone.core.fields.StringField\n",
       "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
       "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
       "    created_at:       fiftyone.core.fields.DateTimeField\n",
       "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
       "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset from fifty one zoo\n",
    "DATASETNAME = \"mnist\"\n",
    "dataset = foz.load_zoo_dataset(DATASETNAME)\n",
    "display(dataset.get_annotation_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fb4e2c3-b346-4212-950d-0daa04b11f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:     mnist\n",
      "Media type:  image\n",
      "Num samples: 10000\n",
      "Sample fields:\n",
      "    id:               fiftyone.core.fields.ObjectIdField\n",
      "    filepath:         fiftyone.core.fields.StringField\n",
      "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
      "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
      "    created_at:       fiftyone.core.fields.DateTimeField\n",
      "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
      "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
      "View stages:\n",
      "    1. MatchTags(tags=['test'], bool=True, all=False)\n"
     ]
    }
   ],
   "source": [
    "# obtain the split of dataset we want to visualize using the fiftyone tags.\n",
    "data_split = dataset.match_tags(\"test\")\n",
    "print(data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa560487",
   "metadata": {},
   "source": [
    "### Read in images from files in fiftyone zoo\n",
    "\n",
    "Get data into a data matrix for use in computing the tSNE embeddings.\n",
    "\n",
    "__NOTE:__ FiftyOne calls the original dataset ($X \\in \\mathbb{R}^{n \\times d}$) ``embeddings`` , __not__ the learned, lower-dimensional embedding. They call the learned embedding ``points``. \n",
    "\n",
    "__NOTE:__ Also, in order for this to work, make sure you have run ``load_datasets.py`` so that datasets are prepared in ``./data`` and the individual image files are properly stored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ef4eee-512b-4ab2-a079-a14d826d6025",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([\n",
    "    cv2.imread(f, cv2.IMREAD_UNCHANGED).ravel() for f in data_split.values(\"filepath\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ee11c8",
   "metadata": {},
   "source": [
    "### Load in the learned embeddings (``points``) from LapTSNE code\n",
    "\n",
    "Either need to now \n",
    "1. Compute these embeddings by leaving the notebook and using \n",
    "```\n",
    "python run.py --config <YOUR CONFIG FILE>\n",
    "```\n",
    "and then loading the learned embedding here into the notebook:\n",
    "```\n",
    "Xlap = np.load(\"./results/<PATH TO LEARNED EMBEDDING>.npy\")\n",
    "```\n",
    "2. Directly call the LapTSNE class here in the notebook:\n",
    "```from run import run_experiment\n",
    "Lap_TSNE = LaplacianTSNE(n_components=m, knn_graph=knn_graph, perplexity=perplexity, k_eigen=k_eigen, approx_nn=approx_nn, learning_rate=learning_rate)\n",
    "Lap_TSNE._prep_graph(X)\n",
    "Xlap, run_time = run_experiment(X, LapTSNE=Lap_TSNE, repulsion_kernel=repulsion_kernel, num_landmarks=num_landmarks, hat_bandwidth=hat_bandwidth)   \n",
    "```\n",
    "3. If want to visualize results already done, simply load the learned embedding as in the second half of option 1.\n",
    "```\n",
    "Xlap = np.load(\"./results/<PATH TO LEARNED EMBEDDING>.npy\")\n",
    "```\n",
    "\n",
    "\n",
    "__NOTE:__ Can repeat this process for various learned LapTSNE embeddings, just repeat the process changing the ``brain_key`` string parameter to uniquely identify each learned embedding that you want to inspect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89ecb503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Example-- load a learned embedding already saved to file\n",
    "Xlap = np.load(\"./results/mnist_test/mnist_test_2_30.0_0_20_50_hat_100_0.5_0.05.npy\")\n",
    "print(Xlap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82282da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lap = fob.compute_visualization(data_split, embeddings=X, num_dims=2, method=\"manual\", points=Xlap,\n",
    "                                   brain_key=\"mnist_test_lap\", verbose=True, seed=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1746af4",
   "metadata": {},
   "source": [
    "#### Explanation of ``fob.compute_visualization``\n",
    "\n",
    "The above function computes/prepares things for running the fiftyone app in the notebook. It requires:\n",
    "* ``data_split`` : the fiftyone ``DatasetView`` object associated with the datapoints we've stored in ``X`` and learned embeddings ``Xlap``\n",
    "* ``embeddings`` : the numpy ndarray of datapoints we refer to as ``X`` (original datapoints)\n",
    "* ``method`` : for loading our embeddings, we need to specify \"manual\", otherwise it computes a default dimensionality reduction algorithm (e.g., UMAP, tSNE)\n",
    "* ``points`` : the numpy ndarray of learned embeddings we refer to as ``Xlap``\n",
    "* ``brain_key`` : the string identifier for this set of learned embeddings in the fiftyone app. \n",
    "\n",
    "__NOTE:__ You can do multiple calls of ``fob.compute_visualization`` for various learned embeddings connected to this ``data_split`` object. Then, when the fiftyone app is called, it will load all of the results connected to ``data_split``, each identified by this ``brain_key`` string value. \n",
    "\n",
    "Below, I've also included an example of running a UMAP visualization (implemented in FiftyOne codebase) for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "402ed6e3-ef72-41e6-afc9-d55e989f24ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ksmiller/miniconda3/envs/tsne2/lib/python3.12/site-packages/sklearn/utils/deprecation.py:146: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/ksmiller/miniconda3/envs/tsne2/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(n_jobs=1, random_state=51, verbose=True)\n",
      "Tue Feb  4 11:49:40 2025 Construct fuzzy simplicial set\n",
      "Tue Feb  4 11:49:40 2025 Finding Nearest Neighbors\n",
      "Tue Feb  4 11:49:40 2025 Building RP forest with 10 trees\n",
      "Tue Feb  4 11:49:42 2025 NN descent for 13 iterations\n",
      "\t 1  /  13\n",
      "\t 2  /  13\n",
      "\t 3  /  13\n",
      "\t 4  /  13\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Tue Feb  4 11:49:46 2025 Finished Nearest Neighbor Search\n",
      "Tue Feb  4 11:49:47 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "010d9a79fbec40c3a48f13adffcfbb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Tue Feb  4 11:49:53 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "include_umap = True\n",
    "if include_umap:\n",
    "    results_umap = fob.compute_visualization(data_split, embeddings=X, num_dims=2, method=\"umap\",\n",
    "                                   brain_key=\"mnist_test_umap\", verbose=True, seed=51)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ab305",
   "metadata": {},
   "source": [
    "# Launch FiftyOne App! \n",
    "\n",
    "Now, we launch the FiftyOne app dashboard here in the notebook to visualize the embeddings. You will initially see just the individual images, but to load the embeddings:\n",
    "1. Click on the ``+`` to add a new tab (window), selecting ``Embeddings`` for this new tab. \n",
    "2. Press the button to split to two tabs (windows). Looks like two rectangles side-by-side\n",
    "3. Over in the right hand window, select ``brain_key`` drop down menu to select the embedding results you wish to visualize. \n",
    "\n",
    "Now, some helpful hints on the visualization tools:\n",
    "1. In the Embeddings window, click on ``Color by`` and select ``ground_truth.label`` and this will color all the embedding points in their respective class labels. \n",
    "2. You can select sets of points with the lasso tool to visualize in the ``Samples`` panel what the original images look like for the subset of embedded points you selected. \n",
    "3. Scroll to zoom in the Embeddings panel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea2da792-84ca-4f45-8bec-b2e46d360d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://localhost:5151/?notebook=True&subscription=56b5a6f3-e694-4fc5-b3dd-9f373fb8eb3f\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x785b0eb9b110>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = fo.launch_app(data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc307e4f-2c74-4d88-9b61-64640f569597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
